{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据，并且只保留常见的前2W个词\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       list([1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 15305, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 18435, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 18665, 594, 7, 5168, 94, 9096, 3987, 15242, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 10161, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 10197, 19, 49, 7, 4, 1885, 13699, 1118, 25, 80, 126, 842, 10, 10, 2, 18223, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 16376, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 13699, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 16376, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 11120, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 11876, 1775, 3353, 12716, 1846, 4, 11286, 7, 154, 5, 4, 518, 53, 13243, 11286, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 18223, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 10359, 9, 242, 4, 91, 1202, 11377, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 18223, 13, 188, 1076, 3222, 19, 4, 13465, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 18223, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 12283, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 15812, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 13713, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]),\n",
       "       list([1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 6397, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 7224, 6, 226, 251, 7, 61, 113])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_word_index = {v:k for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_word_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> shown in australia as <UNK> this incredibly bad movie is so bad that you become <UNK> and have to watch it to the end just to see if it could get any worse and it does the storyline is so predictable it seems written by a high school dramatics class the sets are pathetic but marginally better than the <UNK> and the acting is wooden br br the infant <UNK> seems to have been stolen from the props cupboard of <UNK> <UNK> there didn't seem to be a single original idea in the whole movie br br i found this movie to be so bad that i laughed most of the way through br br malcolm mcdowell should hang his head in shame he obviously needed the money\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   14,   22, ...,    0,    0,    0],\n",
       "       [   1,  194, 1153, ...,    0,    0,    0],\n",
       "       [   1,   14,   47, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   11,    6, ...,    0,    0,    0],\n",
       "       [   1, 1446, 7079, ...,    0,    0,    0],\n",
       "       [   1,   17,    6, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,010,201\n",
      "Trainable params: 2,010,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 一共有2W词\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 100))  # 用100维对词进行嵌入式表示\n",
    "model.add(keras.layers.GlobalAveragePooling1D())  # 对每个词进行平均池化\n",
    "#model.add(keras.layers.GlobalMaxPooling1D())  # 或者是最大池化\n",
    "model.add(keras.layers.Dense(100, activation=tf.nn.relu))  # 通过全连接层\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))  # 通过全连接层得到最终的结果\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用1W的数据作为验证集，其余的数据作为训练集\n",
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 2s 145us/step - loss: 0.6887 - acc: 0.6221 - val_loss: 0.6788 - val_acc: 0.7530\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.6470 - acc: 0.7581 - val_loss: 0.6039 - val_acc: 0.7892\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.5285 - acc: 0.8184 - val_loss: 0.4698 - val_acc: 0.8261\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.3857 - acc: 0.8697 - val_loss: 0.3652 - val_acc: 0.8624\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.2920 - acc: 0.8995 - val_loss: 0.3156 - val_acc: 0.8777\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.2327 - acc: 0.9188 - val_loss: 0.2925 - val_acc: 0.8821\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.1908 - acc: 0.9357 - val_loss: 0.2814 - val_acc: 0.8863\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 98us/step - loss: 0.1594 - acc: 0.9477 - val_loss: 0.2776 - val_acc: 0.8874\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 2s 100us/step - loss: 0.1340 - acc: 0.9590 - val_loss: 0.2784 - val_acc: 0.8884\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.1123 - acc: 0.9697 - val_loss: 0.2811 - val_acc: 0.8902\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 98us/step - loss: 0.0952 - acc: 0.9755 - val_loss: 0.2941 - val_acc: 0.8853\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0806 - acc: 0.9812 - val_loss: 0.2949 - val_acc: 0.8880\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0700 - acc: 0.9843 - val_loss: 0.3047 - val_acc: 0.8859\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.0606 - acc: 0.9869 - val_loss: 0.3133 - val_acc: 0.8850\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0524 - acc: 0.9899 - val_loss: 0.3240 - val_acc: 0.8849\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 98us/step - loss: 0.0459 - acc: 0.9915 - val_loss: 0.3377 - val_acc: 0.8815\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.0396 - acc: 0.9938 - val_loss: 0.3492 - val_acc: 0.8805\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0349 - acc: 0.9944 - val_loss: 0.3579 - val_acc: 0.8812\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 2s 102us/step - loss: 0.0295 - acc: 0.9956 - val_loss: 0.3698 - val_acc: 0.8796\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 2s 100us/step - loss: 0.0256 - acc: 0.9967 - val_loss: 0.3818 - val_acc: 0.8779\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.0227 - acc: 0.9969 - val_loss: 0.3951 - val_acc: 0.8749\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 2s 102us/step - loss: 0.0198 - acc: 0.9979 - val_loss: 0.4042 - val_acc: 0.8759\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0177 - acc: 0.9982 - val_loss: 0.4133 - val_acc: 0.8760\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.0153 - acc: 0.9987 - val_loss: 0.4272 - val_acc: 0.8739\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0134 - acc: 0.9990 - val_loss: 0.4383 - val_acc: 0.8741\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.0113 - acc: 0.9994 - val_loss: 0.4492 - val_acc: 0.8732\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 99us/step - loss: 0.0104 - acc: 0.9992 - val_loss: 0.4593 - val_acc: 0.8731\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.0092 - acc: 0.9997 - val_loss: 0.4720 - val_acc: 0.8718\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 2s 103us/step - loss: 0.0083 - acc: 0.9998 - val_loss: 0.4792 - val_acc: 0.8717\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0078 - acc: 0.9993 - val_loss: 0.4891 - val_acc: 0.8711\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 100us/step - loss: 0.0067 - acc: 0.9999 - val_loss: 0.4963 - val_acc: 0.8713\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 2s 100us/step - loss: 0.0060 - acc: 0.9999 - val_loss: 0.5061 - val_acc: 0.8707\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.0056 - acc: 0.9999 - val_loss: 0.5166 - val_acc: 0.8703\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.0051 - acc: 0.9999 - val_loss: 0.5242 - val_acc: 0.8714\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 2s 105us/step - loss: 0.0046 - acc: 0.9999 - val_loss: 0.5323 - val_acc: 0.8705\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 2s 101us/step - loss: 0.0043 - acc: 0.9999 - val_loss: 0.5399 - val_acc: 0.8709\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 2s 102us/step - loss: 0.0041 - acc: 0.9999 - val_loss: 0.5464 - val_acc: 0.8702\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 2s 100us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.5579 - val_acc: 0.8698\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 2s 100us/step - loss: 0.0036 - acc: 0.9999 - val_loss: 0.5628 - val_acc: 0.8701\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 2s 100us/step - loss: 0.0033 - acc: 0.9999 - val_loss: 0.5708 - val_acc: 0.8707\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 54us/step\n",
      "[0.6176278840708732, 0.857]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每一Epochs都进行F1计算\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from keras.engine.training import Model\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict,average='weighted')\n",
    "        _val_recall = recall_score(val_targ, val_predict,average='weighted')\n",
    "        _val_precision = precision_score(val_targ, val_predict,average='weighted')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print( ' — val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "    \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9999 — val_f1: 0.869390 — val_precision: 0.869421 — val_recall 0.869400\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0030 - acc: 0.9999 - val_loss: 0.5774 - val_acc: 0.8694\n",
      "Epoch 2/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9999 — val_f1: 0.869186 — val_precision: 0.869243 — val_recall 0.869200\n",
      "15000/15000 [==============================] - 2s 127us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.5844 - val_acc: 0.8692\n",
      "Epoch 3/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 1.0000 — val_f1: 0.868784 — val_precision: 0.868853 — val_recall 0.868800\n",
      "15000/15000 [==============================] - 2s 126us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.5902 - val_acc: 0.8688\n",
      "Epoch 4/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9999 — val_f1: 0.869697 — val_precision: 0.869700 — val_recall 0.869700\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.0025 - acc: 0.9999 - val_loss: 0.5959 - val_acc: 0.8697\n",
      "Epoch 5/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 1.0000 — val_f1: 0.868983 — val_precision: 0.869057 — val_recall 0.869000\n",
      "15000/15000 [==============================] - 2s 126us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.6029 - val_acc: 0.8690\n",
      "Epoch 6/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 1.0000 — val_f1: 0.868690 — val_precision: 0.868722 — val_recall 0.868700\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.6086 - val_acc: 0.8687\n",
      "Epoch 7/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 1.0000 — val_f1: 0.868598 — val_precision: 0.868599 — val_recall 0.868600\n",
      "15000/15000 [==============================] - 2s 128us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.6135 - val_acc: 0.8686\n",
      "Epoch 8/90\n",
      "14336/15000 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9999 — val_f1: 0.866304 — val_precision: 0.866368 — val_recall 0.866300\n",
      "15000/15000 [==============================] - 2s 128us/step - loss: 0.0020 - acc: 0.9999 - val_loss: 0.6194 - val_acc: 0.8663\n",
      "Epoch 9/90\n",
      "14336/15000 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 0.9999 — val_f1: 0.866704 — val_precision: 0.866756 — val_recall 0.866700\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.0019 - acc: 0.9999 - val_loss: 0.6240 - val_acc: 0.8667\n",
      "Epoch 10/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 1.0000 — val_f1: 0.866504 — val_precision: 0.866539 — val_recall 0.866500\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.6306 - val_acc: 0.8665\n",
      "Epoch 11/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 1.0000 — val_f1: 0.867581 — val_precision: 0.867668 — val_recall 0.867600\n",
      "15000/15000 [==============================] - 2s 128us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.6393 - val_acc: 0.8676\n",
      "Epoch 12/90\n",
      "14848/15000 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 1.0000 — val_f1: 0.867483 — val_precision: 0.867554 — val_recall 0.867500\n",
      "15000/15000 [==============================] - 2s 128us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.6440 - val_acc: 0.8675\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystopping=keras.callbacks.EarlyStopping(monitor='val_acc', patience=8, verbose=0, mode='max')\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=90,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[metrics,earlystopping],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0009017923107060294, 1.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(partial_x_train, partial_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6439755359441042, 0.8675]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25813128976596517, 0.947]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6967365902328492, 0.85476]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, None, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,080,501\n",
      "Trainable params: 2,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 一共有2W词\n",
    "vocab_size = 20000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 100))  # 用100维对词进行嵌入式表示\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.LSTM(64))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.LSTM(32))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(keras.layers.Dropout(0.5))\n",
    "# model.add(keras.layers.GlobalAveragePooling1D())  # 对每个词进行平均池化\n",
    "#model.add(keras.layers.GlobalMaxPooling1D())  # 或者是最大池化\n",
    "# model.add(keras.layers.Dense(100, activation=tf.nn.relu))  # 通过全连接层\n",
    "# model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))  # 通过全连接层得到最终的结果\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 34s 2ms/step - loss: 0.6924 - acc: 0.5237 - val_loss: 0.6907 - val_acc: 0.5314\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6838 - acc: 0.5760 - val_loss: 0.6779 - val_acc: 0.5569\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6315 - acc: 0.6355 - val_loss: 0.6300 - val_acc: 0.7548\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6223 - acc: 0.6441 - val_loss: 0.6027 - val_acc: 0.7583\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.4986 - acc: 0.7905 - val_loss: 0.5143 - val_acc: 0.7882\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.4652 - acc: 0.8271 - val_loss: 1.4270 - val_acc: 0.6309\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 1.2104 - acc: 0.5663 - val_loss: 0.7288 - val_acc: 0.5053\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.7191 - acc: 0.5126 - val_loss: 0.6904 - val_acc: 0.5258\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6875 - acc: 0.5372 - val_loss: 0.6819 - val_acc: 0.5522\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6798 - acc: 0.5463 - val_loss: 0.6792 - val_acc: 0.5524\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6738 - acc: 0.5575 - val_loss: 0.6763 - val_acc: 0.5582\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 33s 2ms/step - loss: 0.6677 - acc: 0.5710 - val_loss: 0.6740 - val_acc: 0.5600\n",
      "Epoch 13/20\n",
      " 4096/15000 [=======>......................] - ETA: 19s - loss: 0.6622 - acc: 0.5852"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
